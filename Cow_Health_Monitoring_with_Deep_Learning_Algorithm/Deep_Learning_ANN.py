# -*- coding: utf-8 -*-
"""ANN_with_merged_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eYMo0L4AHGffV6ACsCO-2msgkPNB4c4u

# **IoT-Based Cow Health Monitoring Using Deep Learning**

This notebook presents the development of a deep learning-based classification model for monitoring cow health and behavior using IoT sensor data.
The goal is to predict whether a cow is ruminating or not based on accelerometer (Ax, Ay, Az) and gyroscope (Gx, Gy, Gz) readings collected from wearable sensors.

A PyTorch-based Artificial Neural Network (ANN) was implemented to analyze sensor readings and classify cow movements.
The model was trained using Binary Cross-Entropy Loss (BCELoss) and optimized using Adam.
Various data preprocessing, feature engineering, and model optimization techniques were applied to improve performance.

Despite extensive tuning, the model did not achieve high accuracy, and this notebook analyzes the possible reasons behind it while suggesting improvements.

## Why is the Accuracy Low?

Despite improvements, the model's accuracy remains suboptimal, and several key factors contribute to this issue:

1. The dataset has more non-ruminating movements than ruminating ones.

The model leans toward predicting the majority class, leading to low precision for ruminating cows.

Fix Attempted: Used SMOTE to address class imbalance, but further tuning is needed.

2. Overlapping Sensor Data

Some movements (e.g., standing vs. ruminating) have similar sensor readings.

The model struggles to differentiate between them, leading to misclassifications.

3. Learning Rate Might Be Too Low  (lr = 0.0005)

A very small learning rate causes slow weight updates, leading to slow improvement in loss.

Fix Attempted: Increasing lr=0.001 could help the model converge faster.

4. Limited Model Complexity

The model might lack the complexity needed to capture hidden patterns in sensor data.

Fix Attempted: Increasing neurons/layers improved performance slightly, but more feature engineering may be required.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive')

"""# Data Preprocessing

## Loads the dataset from the specified file path.
"""

df = pd.read_csv("/content/drive/MyDrive/Komilo_proj/Datasets/My_Datasets/ANN_with_merged_data.csv", header=0)

"""## Renames the columns for clarity.

Ax, Ay, Az → Accelerometer readings (X, Y, Z axes).

Gx, Gy, Gz → Gyroscope readings (X, Y, Z axes).

Movement → Label for activity classification.

# Feature Engineering
"""

df.columns = ['Time', 'Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz', 'Movement']

""" ## Removes the first column from dataset as the first column was an unnecessary index (or) timestamp."""

df = df.iloc[:, 1:]

"""## The dataset contains accelerometer (Ax, Ay, Az), gyroscope (Gx, Gy, Gz), and a Movement column, which represents activity labels (eating, standing, walking, sleeping, sitting, ruminating)."""

df.head()

"""## Information of the dataset include:

**Dataset Size:** 1,368,929 rows and 7 columns

**Data Types:**

6 numerical columns (float64): Sensor readings (Ax, Ay, Az, Gx, Gy, Gz).

1 categorical column (object): Movement, representing activity labels.

**Missing Values:**

Movement column has 4 missing values (1,188,925 non-null vs. 1,188,929 total rows).
No missing values in the sensor data columns.

**Memory Usage:**

The dataset occupies 73.1 MB, which is manageable for analysis and model training.
"""

df.info()

"""## Observation of dataset include:

**Dataset Overview:**

The dataset contains 1,188,089 sensor readings for each column (Ax, Ay, Az, Gx, Gy, Gz).

No missing values in the numerical columns.

**Mean & Standard Deviation:**

Mean values are close to zero, which is expected for sensor data after calibration.

Standard deviation (std) varies across columns, indicating different ranges of motion.

**Min & Max Values (Range of Data):**

The acceleration (Ax, Ay, Az) values range from -9.93 to 7.14, suggesting possible free-fall or rapid motion.

The gyroscope (Gx, Gy, Gz) values range from -412.78 to 413.13, indicating significant rotational motion changes.

**Interquartile Range (25%, 50%, 75% Percentiles):**

The middle 50% of data is concentrated between small values, suggesting that for most cases, movements are subtle.

Outliers in Max and Min values indicate possible sudden movements or anomalies.

"""

df.describe()

"""##  Remove all rows that contain missing values (NaN).

As an **alternative approach** (Instead of Dropping Rows):

If the missing values are few, dropna() is fine.

If missing values are significant, consider filling them using:

Forward fill (copy previous value): df.fillna(method='ffill')

Backward fill (copy next value): df.fillna(method='bfill')

Replace with most common label (for categorical Movement column) by using
 "df['Movement'].fillna(df['Movement'].mode()[0], inplace=True)"

"""

df = df.dropna()  # Remove missing values (or fill using df.fillna())

"""## Remove duplicate rows from the dataset, keeping only the first occurrence.

This helps clean redundant data, which might be caused by sensor logging errors or data merging issues.
"""

df = df.drop_duplicates()

"""## Confirm that missing data handling (dropna or fillna) was successful or not."""

print(df.isnull().sum())

"""## Printing out the DataFrame"""

df

"""## Check Unique Labels in Movement Column
The Movement column should contain distinct activity labels, but running .unique() reveals inconsistencies.
"""

unique_Movement = df['Movement'].unique()

unique_Movement

""" ## Identifying & Fixing Label Issues

 Correct labels: 'Standing', 'Walking', 'Ruminating', 'Sitting', 'Sleeping', 'Eating'
"""

df = df[df["Movement"].notna()]  # Remove NaN values
df = df[df["Movement"].str.strip() != ""]  # Remove empty spaces

df['Movement'] = df['Movement'].str.strip()

df['Movement'] = df['Movement'].str.title()

df['Movement'] = df['Movement'].replace({
    'Drinking': 'Eating',    # Merge 'Drinking' into 'Eating'
    "Walking'": 'Walking',  # Fix incorrect label
    'walking': 'Walking',
    'Sittting' : 'Sitting', # Ensure consistency with case
    'Reminating' : 'Ruminating',
    'Runibating' : 'Ruminating',
    'Movnig' : 'Walking',
    'Anding' : 'Eating',
    'Rumninating' : 'Ruminating',
    'Siiting' : 'Sitting',
    'Sleepng' : 'Sleeping',
    'Moving' : 'Walking'
})

"""## Verify Cleaned Labels
After preprocessing, check the unique labels again to confirm corrections:
"""

print(df['Movement'].unique())

df.Movement.value_counts(dropna=False)

df['Movement'].isna().sum()

"""## Dataset Overview
After cleaning the data by handling missing values and duplicates, the final dataset contains:

**Total Rows:** 1,188,084

**Total Columns:** 7 (Ax, Ay, Az, Gx, Gy, Gz, Movement)

This dataset consists of accelerometer and gyroscope readings, along with labeled movement types (standing, sitting, moving, etc.). These features will be used for analyzing movement pattern (ruminating) and building a deep learning model for classification.


"""

print(f"\nTotal rows: {len(df)}")

"""## Movement Distribution Analysis

The bar chart shows the distribution of different movement categories in the dataset. Key observations:

**Most Frequent Movements:**

Ruminating has the highest count (~350,000), followed by Standing, Sitting, and Eating.
These activities are common in cattle behavior, as cows spend a significant portion of their time ruminating and resting.

**Less Frequent Movements:**

Sleeping and Walking have the lowest counts.
This could indicate either less recorded data for these activities or natural behavior where cows walk and sleep less frequently compared to other activities.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the distribution of the Movement column
df["Movement"].value_counts().plot(kind='bar', title="Distribution of Movements", color='skyblue')

plt.xlabel("Movements")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

"""## Movement Class Distribution Analysis

The count plot visualizes the distribution of movement categories in the dataset.

Most Common Movements:

Ruminating is the most frequent activity (~350,000 instances).
Standing, Sitting, and Eating also have a significant number of samples.

Least Common Movements:

Walking and Sleeping have noticeably fewer occurrences.

**This suggests a potential class imbalance, which may affect model performance if not addressed.**
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot class distribution
sns.countplot(x="Movement", data=df)
plt.show()

"""## Preparing for One-Hot Encoding

Before applying one-hot encoding, confirm that the dataset contains:


*    6 numerical sensor columns: Ax, Ay, Az, Gx, Gy, Gz (Accelerometer & Gyroscope readings)List item
*   1 categorical column: Movement (which needs encoding)

## Why One-Hot Encoding?
The Movement column is categorical (object type) and needs to be converted into numerical format for machine learning models.

One-Hot Encoding creates separate binary columns for each movement category (e.g., Standing → [1,0,0,0], Walking → [0,1,0,0]).











"""

print("Before One-Hot Encoding:")
print(df.columns)  # Ensure all sensor columns (Ax, Ay, Az, Gx, Gy, Gz) are present

"""## One-Hot Encoding Applied Successfully
Now, the categorical Movement column has been transformed into separate binary columns, representing each movement type:



*   Sensor Data Columns Remain Unchanged:
Ax, Ay, Az, Gx, Gy, Gz (Accelerometer & Gyroscope readings).

*   New One-Hot Encoded Columns:

Movement_Eating
Movement_Ruminating
Movement_Sitting
Movement_Sleeping
Movement_Standing
Movement_Walking

Each row now has a 1 or 0 in these columns, indicating the movement type for that instance.
"""

df_encoded = pd.get_dummies(df, columns=['Movement'], drop_first=False)
print("After One-Hot Encoding:")
print(df_encoded.columns)  # Ensure sensor columns still exist

print(df.head())  # Ensure Movement and sensor columns exist before encoding

print(df_encoded.head())  # Check the dataset after encoding

"""## Data Overview After Encoding

The df_encoded.info( ) output confirms:



*  Total Rows: 1,188,084 (No data loss after encoding).
*   Sensor Data Columns (float64): Ax, Ay, Az, Gx, Gy, Gz (Accelerometer & Gyroscope readings).

*   Encoded Movement Columns (bool):

Movement_Eating, Movement_Ruminating, Movement_Sitting, Movement_Sleeping, Movement_Standing, Movement_Walking.
These binary columns indicate the presence (True) or absence (False) of each movement type.

*   No Missing Values – The dataset is now fully preprocessed and ready for modeling.
"""

print(df_encoded.info())  # Check data types and missing values

print(df_encoded.describe())  # Summary statistics of numerical features

""" ## Sensor Data Distribution Analysis

 The histograms display the distribution of accelerometer (Ax, Ay, Az) and gyroscope (Gx, Gy, Gz) readings.

 Key Observations:

*    Accelerometer Readings (Ax, Ay, Az)

Most values are centered around 0, indicating normal movement.
Ax has a right-skewed distribution, suggesting some extreme positive values.
*   Gyroscope Readings (Gx, Gy, Gz)

Gx has a strong peak near 0, with some high-magnitude values, possibly from sudden movements.
Gz has a very sharp peak, suggesting minimal variation in certain movements.
*   Potential Outliers:

Some axes (e.g., Ax, Gx) show long tails, indicating possible outliers from sudden changes in motion.
Preprocessing (e.g., clipping extreme values or normalization) may be necessary before training a machine learning model.

"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot distributions of sensor data
df_encoded[['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']].hist(figsize=(12, 8), bins=30)
plt.show()

"""## Outlier Removal Using Z-Score

This code removes outliers in sensor data (Ax, Ay, Az, Gx, Gy, Gz) using Z-score analysis.


1.  Computes Z-scores:

Standardizes sensor values to measure how far they deviate from the mean.
2.  Filters Outliers:

Keeps only rows where all sensor values have a Z-score between -3 and 3 (within 3 standard deviations).
This removes extreme outliers that could negatively impact machine learning performance.
"""

from scipy.stats import zscore

# Compute z-scores for sensor data
z_scores = df_encoded[['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']].apply(zscore)

# Define threshold (remove values beyond 3 standard deviations)
df_encoded = df_encoded[(z_scores.abs() < 3).all(axis=1)]

"""## Sensor Data Distribution After Outlier Removal


*  More Normalized Distributions:

The data is now more centered around 0, reducing extreme skewness.
Ax, Ay, Az show a more balanced spread compared to before.
*   Reduced Extreme Values:

Previously, Gx, Gy, Gz had long tails and extreme spikes.
Now, values are within a reasonable range, making them more suitable for machine learning models.
*   Smoothed Gyroscope Distributions:

Gyroscope readings (Gx, Gy, Gz) previously had high peaks near 0 with extreme values.
The outlier removal process has kept the key distribution shape while reducing noise.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Plot distributions of sensor data
df_encoded[['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']].hist(figsize=(12, 8), bins=30)
plt.show()

"""## Statistics for Ax (After Outlier Removal)


*  Count: 1,105,286 rows (indicating some data was removed due to outlier filtering).
*   Mean: 0.77, suggesting a slight right-skew in the distribution.
*  Standard Deviation (std): 0.34, indicating moderate variation in the data.
*   Range:
Min: -0.55,
Max: 2.07

The values now fall within a reasonable range compared to before outlier removal.
*   Interquartile Range (IQR):
25%: 0.79, 75%: 0.97 → Most values are tightly clustered.
"""

print(df_encoded['Ax'].describe())

"""## Key Observations for Az (Accelerometer Z-Axis):


*   Mean: 0.18, indicating a more balanced distribution.
*   Standard Deviation: 0.39, slightly more dispersed than Ax.
*   Range:Min: -0.97, Max: 1.22

The values are much more controlled compared to earlier distributions.

"""

print(df_encoded['Az'].describe())

import seaborn as sns
import matplotlib.pyplot as plt

# Plot distributions of sensor data
df_encoded[['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']].hist(figsize=(12, 8), bins=30)
plt.show()

"""## Outlier Removal Using IQR & Sensor Data Distribution Analysis

Implementation of IQR-based outlier detection for Ax as z-score didn't work out very much effectively.

1. Calculate Quartiles:
2. Define Bounds for Outliers:Any value outside these bounds is considered an outlier.
3. Filter Data:Removes rows where Ax values fall outside these bounds.

 This step ensures the dataset retains meaningful sensor readings.
"""

import numpy as np

# Compute IQR for Ax
Q1 = df_encoded['Ax'].quantile(0.25)
Q3 = df_encoded['Ax'].quantile(0.75)
IQR = Q3 - Q1

# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
df_encoded = df_encoded[(df_encoded['Ax'] >= lower_bound) & (df_encoded['Ax'] <= upper_bound)]

sns.countplot(x="Movement", data=df)  # Original 'Movement' column before encoding
plt.xticks(rotation=45)
plt.title("Class Distribution of Movement Labels")
plt.show()

df_encoded.iloc[:, 6:].sum().plot(kind='bar', title="Class Distribution (One-Hot Encoded)")

"""## Correlation Between Movement Types and Sensor Features
This heatmap shows the correlation between sensor readings (Ax, Ay, Az, Gx, Gy, Gz) and movement types. Correlation values range from -1 to 1:

*  Ay Shows the Strongest Correlations:

Movement_Eating (0.06) and Movement_Standing (-0.09) suggest that lateral acceleration (Ay) plays a role in distinguishing these movements.

This indicates that Ay might be a useful feature in classifying movement types.


*  Weak Correlations for Other Sensors:

Ax, Az, Gx, Gy, Gz show very low correlation values (close to 0), suggesting that these features alone may not strongly predict movement types.

However, they might still contribute to multivariate classification models (e.g., Random Forest, Neural Networks).


"""

correlation_matrix = df_encoded.corr()

# Select only movement and sensor data
movement_columns = [col for col in df_encoded.columns if "Movement_" in col]
sensor_columns = ["Ax", "Ay", "Az", "Gx", "Gy", "Gz"]

correlation_subset = correlation_matrix.loc[movement_columns, sensor_columns]

# Plot heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_subset, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Between Movement and Sensor Features")
plt.show()

"""## Correlation Between Engineered Features and Movement Types
This heatmap shows the correlation between newly engineered features:


*  Accel_Magnitude – Overall acceleration magnitude
*   Gyro_Magnitude – Overall gyroscope magnitude

and different movement types.

**Observations:**

*    Low Correlation with Movements

Both Accel_Magnitude and Gyro_Magnitude have very weak correlations with movement types (values close to 0).
This suggests that raw acceleration and gyroscope magnitudes alone may not be strong indicators of movement classification.
*  Slight Positive Correlation with Movement_Standing

Accel_Magnitude has a small positive correlation (0.04) with Standing, which might suggest that standing involves stable but noticeable acceleration.
*   Gyro_Magnitude and Accel_Magnitude Are Slightly Related (0.10)

This makes sense because movement generally involves both acceleration and rotational motion.
"""

import numpy as np

# Compute sensor magnitude as a new feature
df_encoded['Accel_Magnitude'] = np.sqrt(df_encoded['Ax']**2 + df_encoded['Ay']**2 + df_encoded['Az']**2)
df_encoded['Gyro_Magnitude'] = np.sqrt(df_encoded['Gx']**2 + df_encoded['Gy']**2 + df_encoded['Gz']**2)

# Check updated correlation with new features
correlation_matrix = df_encoded.corr()
correlation_subset = correlation_matrix.loc[df_encoded.iloc[:, 6:].columns, ['Accel_Magnitude', 'Gyro_Magnitude']]

sns.heatmap(correlation_subset, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Between New Features and Movement")
plt.show()

"""## Importing Necessary Libraries


*   torch, torch.nn, torch.optim → PyTorch for building and training the neural network.
*   train_test_split → Splitting data into training and testing sets.
*  MinMaxScaler → Normalizing sensor values for better model convergence.
*  SMOTE → Handles class imbalance if one class has much fewer samples.







"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE  # Use as class imbalance exists

"""# Data Splitting & Normalization

## Setting Up Data for a Binary Classification Model (Rumination Detection)
This code prepares data for training a deep learning model using PyTorch to classify whether a cow is ruminating (healthy) or not (potentially unhealthy).

## Selecting Features (X)

Uses only sensor data (Ax, Ay, Az, Gx, Gy, Gz).

These features will be used to predict the cow's movement state.

## Defining the Target (y)

Binary classification problem:

1 → The cow is ruminating (healthy).

0 → The cow is not ruminating (potentially unhealthy).

This is achieved by converting the Movement_Ruminating one-hot encoded column into a binary label (astype(int)).
"""

#Step 1: Select features (sensor data only) and target (binary label)
X = df_encoded[['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']]
y = (df_encoded['Movement_Ruminating'] == 1).astype(int)  # 1 = Healthy, 0 = Unhealthy

print(y.value_counts())  # Ensure balanced classes

"""## Feature Scaling Using MinMaxScaler
Normalizes the sensor data (X) using MinMaxScaler, which scales values between 0 and 1 in order to


*   Ensures all features have the same scale → Prevents large-magnitude features (e.g., gyroscope readings) from dominating small-magnitude features (e.g., accelerometer).
*   Improves convergence in deep learning models → Neural networks perform better when inputs are within a similar range.
*   Preserves Relationships Between Data Points → Unlike standardization (which transforms data to have mean=0 and std=1), MinMaxScaler keeps the original distribution but scales it within a fixed range.


"""

#Step 2: Normalize sensor data (scale between 0 and 1)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

"""## Splitting Data into Training and Testing Sets
This code splits the normalized sensor data (X_scaled) and labels (y) into training and test sets for model evaluation.
"""

#Step 3: Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)

"""## Class Imbalance Detected in the Dataset
The dataset contains:
*  655,324 samples (Class 0 - Not Ruminating)
*  257,615 samples (Class 1 - Ruminating)

This shows a class imbalance, where the "Not Ruminating" class has ~2.5 times more samples than the "Ruminating" class. If not handled, this imbalance may cause the machine learning model to be biased towards the majority class (Not Ruminating).
"""

#Step 4: Apply SMOTE (ONLY on the training set if the dataset is imbalanced)
# Apply SMOTE only to the training set
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Class distribution after SMOTE:", np.bincount(y_train_resampled))

"""Try Class Weights in the Neural Network

Instead of oversampling, assign higher loss weights to the minority class:

from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight('balanced', classes=[0, 1], y=y_train.numpy())
class_weights = torch.tensor(class_weights, dtype=torch.float)

loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])  # Adjusts weight for minority class

## Converting Data to PyTorch Tensors for Neural Network Training
Converts the training and testing datasets into PyTorch tensors, which are required for deep learning models.



*  Tensors are required for PyTorch models – PyTorch operates on tensors instead of Pandas/NumPy arrays.
*    Ensures correct data format for Neural Networks – .view(-1,1) reshapes the labels for compatibility with PyTorch's loss functions.
*   Maintains numerical precision – Using float32 prevents errors related to data type mismatches.
"""

#Step 5: Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_resampled.values, dtype=torch.float32).view(-1, 1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

"""## Check Training & Testing Data Shape
This output confirms that the training and testing datasets have been correctly split and converted into PyTorch tensors.

**Observations:**


*   Training Data Shape: 1,048,518 samples with 6 features (sensor readings: Ax, Ay, Az, Gx, Gy, Gz).

*   Testing Data Shape: 182,588 samples used for evaluating model performance.
"""

print("Training data shape:", X_train_tensor.shape)
print("Testing data shape:", X_test_tensor.shape)

"""## Preparing Data for Training
Before training the model, it is needed to efficiently handle the dataset. PyTorch provides TensorDataset and DataLoader to manage the training and testing data in batches.

*   Convert the Training & Testing Data into PyTorch Datasets.
*   Create DataLoader for Mini-Batch Processing.
*   Keeps test data order fixed for consistent evaluation.




"""

from torch.utils.data import TensorDataset, DataLoader

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

"""# Building the Neural Network Model for Cow Health Classification

Define a 4-layer feedforward neural network using PyTorch’s nn.Module. This model will learn to classify movement patterns based on accelerometer (Ax, Ay, Az) and gyroscope (Gx, Gy, Gz) sensor readings.

Features of ANN:


1.   Input Layer

Takes in input_size features (6 sensor readings: Ax, Ay, Az, Gx, Gy, Gz).
2.   Hidden Layers

First Layer: 128 neurons, followed by Leaky ReLU activation to avoid dead neurons.

Second Layer: 64 neurons, using Leaky ReLU.

Third Layer: 32 neurons, using Leaky ReLU.
3. Output Layer

1 neuron (binary classification: ruminating (1) vs. not ruminating (0)).

Sigmoid activation function → Ensures output is between 0 and 1 (probability).
4. Forward Propagation (forward method): Data passes through the layers, applying activations at each step.
Uses Leaky ReLU for hidden layers and Sigmoid for the output layer.

"""

class CowHealthANN(nn.Module):
    def __init__(self, input_size):
        super(CowHealthANN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)  # Increased neurons
        self.relu = nn.LeakyReLU(0.01)  # LeakyReLU to prevent dead neurons
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.sigmoid(self.fc4(x))
        return x

"""## Instantiating and Displaying the ANN Model
Initializes the CowHealthANN model and prints its architecture. It ensures that the neural network is correctly set up before training.
"""

# Instantiate model
model = CowHealthANN(input_size=X_train_tensor.shape[1])
print(model)

"""# Training the Model

## Defining the Loss Function & Optimizer for Training
Define the loss function (how well the model performs) and the optimizer (how the model updates its weights).
"""

criterion = nn.BCELoss()  # Binary Cross-Entropy Loss

optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)

"""## Training the Neural Network (Forward & Backpropagation)
Update the model’s weights over multiple epochs to minimize the loss function.
"""

# Training loop
epochs = 100  # Adjust as needed
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()  # Reset gradients
    y_pred = model(X_train_tensor)  # Forward pass
    loss = criterion(y_pred, y_train_tensor)  # Compute loss
    loss.backward()  # Backpropagation
    optimizer.step()  # Update weights

    # Print loss every 10 epochs
    if (epoch+1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

"""# Evaluating Model Performance on the Test Set

Model Evaluation & Accuracy Calculation measures how well the trained model generalizes to unseen test data.


"""

# Evaluate model
model.eval()
with torch.no_grad():
    y_test_pred = model(X_test_tensor)
    y_test_pred = (y_test_pred > 0.5).float()  # Convert probabilities to binary labels

# Compute accuracy
accuracy = (y_test_pred.eq(y_test_tensor).sum() / y_test_tensor.shape[0]).item()
print(f"Test Accuracy: {accuracy:.4f}")

"""# Visualizing Training Loss Curve

Analyze how well the model is learning during training by tracking the loss over epochs.
1. Collects Training Loss Over Epochs
2. Trains the Model and Updates Weights
3. Plots the Training Loss


"""

import matplotlib.pyplot as plt

losses = []
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    y_pred = model(X_train_tensor)
    loss = criterion(y_pred, y_train_tensor)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())

# Plot training loss
plt.plot(losses, label="Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training Loss Curve")
plt.show()

""" ## Evaluating Model Performance Using Confusion Matrix


"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute confusion matrix
cm = confusion_matrix(y_test_tensor.numpy(), y_test_pred.numpy())

# Display
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix")
plt.show()